{
 "metadata": {
  "name": "",
  "signature": "sha256:3e08c462067fd3e8f862c5fc43cf2d1a44eda7aa557c28be1bdb49d9af8bd43a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "http://nbviewer.ipython.org/gist/rjweiss/7577004"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "#from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.preprocessing import LabelEncoder \n",
      "from sklearn import cross_validation\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import re\n",
      "import nltk\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "from bs4 import BeautifulSoup\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "class stemmerUtility(object):\n",
      "    \"\"\"Stemming functionality\"\"\"\n",
      "    @staticmethod\n",
      "    def stemPorter(review_text):\n",
      "        porter = PorterStemmer()\n",
      "        preprocessed_docs = []\n",
      "        for doc in review_text:\n",
      "            final_doc = []\n",
      "            for word in doc:\n",
      "                final_doc.append(porter.stem(word))\n",
      "                #final_doc.append(snowball.stem(word))\n",
      "                #final_doc.append(wordnet.lemmatize(word)) #note that lemmatize() can also takes part of speech as an argument!\n",
      "            preprocessed_docs.append(final_doc)\n",
      "        return preprocessed_docs\n",
      "\n",
      "\n",
      "class KaggleWord2VecUtility(object):\n",
      "    \"\"\"KaggleWord2VecUtility is a utility class for processing raw HTML text into segments for further learning\"\"\"\n",
      "\n",
      "    @staticmethod\n",
      "    def review_to_wordlist( review, remove_stopwords=False ):\n",
      "        # 1. Remove HTML\n",
      "        review_text = BeautifulSoup(review).get_text()\n",
      "        # 2. Remove non-letters\n",
      "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
      "        # 2.1 Remove single letters\n",
      "        review_text = re.sub('/(?<!\\S).(?!\\S)\\s*/', '', review_text);\n",
      "        # 3. Convert words to lower case and split them\n",
      "        words = review_text.lower().split()\n",
      "        newwords=[]\n",
      "        for word in words:\n",
      "            if len(word)>2:\n",
      "                newwords.append(word)\n",
      "        # 4. Optionally remove stop words (false by default)\n",
      "        if remove_stopwords:\n",
      "            stops = set(stopwords.words(\"english\"))\n",
      "            newwords = [w for w in newwords if not w in stops]\n",
      "        #\n",
      "        # 5. Return a list of words\n",
      "        return(newwords)\n",
      "\n",
      "    # Define a function to split a review into parsed sentences\n",
      "    @staticmethod\n",
      "    def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
      "        # Function to split a review into parsed sentences. Returns a\n",
      "        # list of sentences, where each sentence is a list of words\n",
      "        #\n",
      "        # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
      "        raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
      "        #\n",
      "        # 2. Loop over each sentence\n",
      "        sentences = []\n",
      "        for raw_sentence in raw_sentences:\n",
      "            # If a sentence is empty, skip it\n",
      "            if len(raw_sentence) > 0:\n",
      "                # Otherwise, call review_to_wordlist to get a list of words\n",
      "                sentences.append( KaggleWord2VecUtility.review_to_wordlist( raw_sentence, \\\n",
      "                  remove_stopwords ))\n",
      "        #\n",
      "        # Return the list of sentences (each sentence is a list of words,\n",
      "        # so this returns a list of lists\n",
      "        return sentences\n",
      "    \n",
      "train = pd.read_csv(\"/Users/taposh/workspace/kaggle/bow/labeledTrainData.tsv\", header=0,delimiter=\"\\t\", quoting=3)\n",
      "test = pd.read_csv(\"/Users/taposh/workspace/kaggle/bow/testData.tsv\", header=0, delimiter=\"\\t\",quoting=3)    \n",
      "\n",
      "#train = pd.read_csv(\"/Users/taposh/workspace/kaggle/bow/labeledTrainData.tsv\", header=0, \\\n",
      "#                    delimiter=\"\\t\", quoting=3)\n",
      "#test = pd.read_csv(\"/Users/taposh/workspace/kaggle/bow/testData.tsv\", header=0, delimiter=\"\\t\", \\\n",
      "                   #quoting=3 )\n",
      "y = train[\"sentiment\"]  \n",
      "print(\"Cleaning and parsing movie reviews...\\n\")      \n",
      "traindata = []\n",
      "for i in range( 0, len(train[\"review\"])):\n",
      "#for i in range(0,10):    \n",
      "    traindata.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(train[\"review\"][i], False)))\n",
      "testdata = []\n",
      "####\n",
      "for i in range(0,len(test[\"review\"])):\n",
      "    testdata.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(test[\"review\"][i], False)))\n",
      "    #print(testdata)\n",
      "print ('vectorizing... ',) \n",
      "tfv = TfidfVectorizer(min_df=2,  max_features=None, \n",
      "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
      "        ngram_range=(1, 2), use_idf=2,smooth_idf=1,sublinear_tf=1,\n",
      "        stop_words = 'english')\n",
      "X_all = traindata + testdata\n",
      "lentrain = len(traindata)\n",
      "\n",
      "for jj in range(0,10): \n",
      "print(X_all[jj])\n",
      "\n",
      "print (\"fitting pipeline... \",)\n",
      "tfv.fit(X_all)\n",
      "X_all = tfv.transform(X_all)\n",
      "\n",
      "# RF transform 1st column to numbers\n",
      "#X_all[:,0] = LabelEncoder().fit_transform(X_all[:,0])\n",
      "\n",
      "\n",
      "#for Logit\n",
      "X = X_all[:lentrain]\n",
      "X_test = X_all[lentrain:]\n",
      "\n",
      "\n",
      "#model = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None\n",
      "model = LogisticRegression(penalty='l2', dual=True, tol=0.0001,C=14, fit_intercept=True, intercept_scaling=1,class_weight=None, random_state=None)\n",
      "\n",
      "#model = RandomForestRegressor(n_estimators=150, min_samples_split=1)\n",
      "#model.fit(X, y)\n",
      "#print X\n",
      "#print regressor.predict(X)\n",
      "\n",
      "print(\"25 Fold CV Score: \", np.mean(cross_validation.cross_val_score(model, X, y, cv=36, scoring='roc_auc')))\n",
      "\n",
      "print(\"Retrain on all training data, predicting test labels...\\n\")\n",
      "model.fit(X,y)\n",
      "result = model.predict_proba(X_test)[:,1]\n",
      "#result = model.predict(X_test)\n",
      "print(result)\n",
      "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
      "\n",
      "import csv\n",
      "# Use pandas to write the comma-separated output file\n",
      "output.to_csv('/Users/taposh/workspace/kaggle/bow/Bag_of_Words_model_v17.csv',quoting=3, escapechar=\",\",index=False,encoding='utf-8')\n",
      "#output.to_csv(\"/Users/taposhdr/workspace/decision_science/kaggle/bow/data/Bag_of_Words_model-1.csv\", index=False, quoting=csv.QUOTE_NONE)\n",
      "print(\"Wrote results to Bag_of_Words_model.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cleaning and parsing movie reviews...\n",
        "\n",
        "vectorizing... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "fitting pipeline... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "25 Fold CV Score: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.964641955994\n",
        "Retrain on all training data, predicting test labels...\n",
        "\n",
        "[ 0.98821375  0.02077674  0.57377324 ...,  0.38864432  0.96238562\n",
        "  0.69040803]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wrote results to Bag_of_Words_model.csv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 100
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "c=16 | 25 Fold CV Score:  0.96409904\n",
      "c=17 | 25 Fold CV Score:  0.96408976\n",
      "c=13 | 25 Fold CV Score:  0.96410064\n",
      "c=09 | 25 Fold CV Score:  0.96406416\n",
      "c=08 | 25 Fold CV Score:  0.96402832\n",
      "c=14 | 25 Fold CV Score:  0.96410448\n",
      "c=14 tf=2 | 25 Fold CV Score:  0.96444656*\n",
      "c=14 tf=2 | 35 Fold CV Score:  0.96461634252\n",
      "c=14 tf=2 | 36 Fold CV Score:  0.964697569423\n",
      "c=14 tf=1 | 25 Fold CV Score:  0.96239056\n",
      "c=12 | 25 Fold CV Score:  0.96409408\n",
      "c=11 | 25 Fold CV Score:  0.96408656\n",
      "c=15 | 25 Fold CV Score:  0.9641014\n",
      "c=14.8 | 25 Fold CV Score:  0.96410384\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "#from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.preprocessing import LabelEncoder \n",
      "from sklearn import cross_validation\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import re\n",
      "import nltk\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "class KaggleWord2VecUtility(object):\n",
      "    \"\"\"KaggleWord2VecUtility is a utility class for processing raw HTML text into segments for further learning\"\"\"\n",
      "\n",
      "    @staticmethod\n",
      "    def review_to_wordlist( review, remove_stopwords=False ):\n",
      "        # Function to convert a document to a sequence of words,\n",
      "        # optionally removing stop words.  Returns a list of words.\n",
      "        #\n",
      "        # 1. Remove HTML\n",
      "        review_text = BeautifulSoup(review).get_text()\n",
      "        #\n",
      "        # 2. Remove non-letters\n",
      "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
      "        \n",
      "        # 2.1 Remove single letters\n",
      "        #review_text = re.sub('/(?<!\\S).(?!\\S)\\s*/', '', review_text);\n",
      "        #\n",
      "        # 3. Convert words to lower case and split them\n",
      "        words = review_text.lower().split()\n",
      "        #\n",
      "        # 4. Optionally remove stop words (false by default)\n",
      "        if remove_stopwords:\n",
      "            stops = set(stopwords.words(\"english\"))\n",
      "            words = [w for w in words if not w in stops]\n",
      "        #\n",
      "        # 5. Return a list of words\n",
      "        return(words)\n",
      "\n",
      "    # Define a function to split a review into parsed sentences\n",
      "    @staticmethod\n",
      "    def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
      "        # Function to split a review into parsed sentences. Returns a\n",
      "        # list of sentences, where each sentence is a list of words\n",
      "        #\n",
      "        # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
      "        raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
      "        #\n",
      "        # 2. Loop over each sentence\n",
      "        sentences = []\n",
      "        for raw_sentence in raw_sentences:\n",
      "            # If a sentence is empty, skip it\n",
      "            if len(raw_sentence) > 0:\n",
      "                # Otherwise, call review_to_wordlist to get a list of words\n",
      "                sentences.append( KaggleWord2VecUtility.review_to_wordlist( raw_sentence, \\\n",
      "                  remove_stopwords ))\n",
      "        #\n",
      "        # Return the list of sentences (each sentence is a list of words,\n",
      "        # so this returns a list of lists\n",
      "        return sentences\n",
      "    \n",
      "train = pd.read_csv(\"/Users/taposh/workspace/kaggle/bow/labeledTrainData.tsv\", header=0,delimiter=\"\\t\", quoting=3)\n",
      "test = pd.read_csv(\"/Users/taposh/workspace/kaggle/bow/testData.tsv\", header=0, delimiter=\"\\t\",quoting=3)    \n",
      "\n",
      "#train = pd.read_csv(\"/Users/taposh/workspace/kaggle/bow/labeledTrainData.tsv\", header=0, \\\n",
      "#                    delimiter=\"\\t\", quoting=3)\n",
      "#test = pd.read_csv(\"/Users/taposh/workspace/kaggle/bow/testData.tsv\", header=0, delimiter=\"\\t\", \\\n",
      "                   #quoting=3 )\n",
      "y = train[\"sentiment\"]  \n",
      "print(\"Cleaning and parsing movie reviews...\\n\")      \n",
      "traindata = []\n",
      "for i in range( 0, len(train[\"review\"])):\n",
      "    traindata.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(train[\"review\"][i], False)))\n",
      "testdata = []\n",
      "for i in range(0,len(test[\"review\"])):\n",
      "    testdata.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(test[\"review\"][i], False)))\n",
      "    #print(testdata)\n",
      "print ('vectorizing... ',) \n",
      "tfv = TfidfVectorizer(min_df=6,  max_features=None, \n",
      "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
      "        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
      "        stop_words = 'english')\n",
      "X_all = traindata + testdata\n",
      "lentrain = len(traindata)\n",
      "\n",
      "print (\"fitting pipeline... \",)\n",
      "tfv.fit(X_all)\n",
      "X_all = tfv.transform(X_all)\n",
      "\n",
      "# RF transform 1st column to numbers\n",
      "#X_all[:,0] = LabelEncoder().fit_transform(X_all[:,0])\n",
      "\n",
      "\n",
      "#for Logit\n",
      "X = X_all[:lentrain]\n",
      "X_test = X_all[lentrain:]\n",
      "\n",
      "\n",
      "#model = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None\n",
      "model = LogisticRegression(penalty='l2', dual=True, tol=0.00001,C=14, fit_intercept=True, intercept_scaling=1,class_weight=None, random_state=None)\n",
      "\n",
      "#model = RandomForestRegressor(n_estimators=150, min_samples_split=1)\n",
      "#model.fit(X, y)\n",
      "#print X\n",
      "#print regressor.predict(X)\n",
      "\n",
      "print(\"25 Fold CV Score: \", np.mean(cross_validation.cross_val_score(model, X, y, cv=25, scoring='roc_auc')))\n",
      "\n",
      "print(\"Retrain on all training data, predicting test labels...\\n\")\n",
      "model.fit(X,y)\n",
      "result = model.predict_proba(X_test)[:,1]\n",
      "#result = model.predict(X_test)\n",
      "print(result)\n",
      "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
      "\n",
      "import csv\n",
      "# Use pandas to write the comma-separated output file\n",
      "output.to_csv('/Users/taposh/workspace/kaggle/bow/Bag_of_Words_model_v14_8.csv',quoting=3, escapechar=\",\",index=False,encoding='utf-8')\n",
      "#output.to_csv(\"/Users/taposhdr/workspace/decision_science/kaggle/bow/data/Bag_of_Words_model-1.csv\", index=False, quoting=csv.QUOTE_NONE)\n",
      "print(\"Wrote results to Bag_of_Words_model.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cleaning and parsing movie reviews...\n",
        "\n",
        "vectorizing... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "fitting pipeline... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "25 Fold CV Score: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.96347984\n",
        "Retrain on all training data, predicting test labels...\n",
        "\n",
        "[ 0.99295209  0.01453209  0.50232769 ...,  0.38160522  0.9631343\n",
        "  0.63780148]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wrote results to Bag_of_Words_model.csv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "#from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.preprocessing import LabelEncoder \n",
      "from sklearn import cross_validation\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import re\n",
      "import nltk\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "class KaggleWord2VecUtility(object):\n",
      "    \"\"\"KaggleWord2VecUtility is a utility class for processing raw HTML text into segments for further learning\"\"\"\n",
      "\n",
      "    @staticmethod\n",
      "    def review_to_wordlist( review, remove_stopwords=False ):\n",
      "        # Function to convert a document to a sequence of words,\n",
      "        # optionally removing stop words.  Returns a list of words.\n",
      "        #\n",
      "        # 1. Remove HTML\n",
      "        review_text = BeautifulSoup(review).get_text()\n",
      "        #\n",
      "        # 2. Remove non-letters\n",
      "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
      "        \n",
      "        # 2.1 Remove single letters\n",
      "        #review_text = re.sub('/(?<!\\S).(?!\\S)\\s*/', '', review_text);\n",
      "        #\n",
      "        # 3. Convert words to lower case and split them\n",
      "        words = review_text.lower().split()\n",
      "        #\n",
      "        # 4. Optionally remove stop words (false by default)\n",
      "        if remove_stopwords:\n",
      "            stops = set(stopwords.words(\"english\"))\n",
      "            words = [w for w in words if not w in stops]\n",
      "        #\n",
      "        # 5. Return a list of words\n",
      "        return(words)\n",
      "\n",
      "    # Define a function to split a review into parsed sentences\n",
      "    @staticmethod\n",
      "    def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
      "        # Function to split a review into parsed sentences. Returns a\n",
      "        # list of sentences, where each sentence is a list of words\n",
      "        #\n",
      "        # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
      "        raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
      "        #\n",
      "        # 2. Loop over each sentence\n",
      "        sentences = []\n",
      "        for raw_sentence in raw_sentences:\n",
      "            # If a sentence is empty, skip it\n",
      "            if len(raw_sentence) > 0:\n",
      "                # Otherwise, call review_to_wordlist to get a list of words\n",
      "                sentences.append( KaggleWord2VecUtility.review_to_wordlist( raw_sentence, \\\n",
      "                  remove_stopwords ))\n",
      "        #\n",
      "        # Return the list of sentences (each sentence is a list of words,\n",
      "        # so this returns a list of lists\n",
      "        return sentences\n",
      "    \n",
      "train = pd.read_csv(\"/Users/taposh/workspace/kaggle/bow/labeledTrainData.tsv\", header=0,delimiter=\"\\t\", quoting=3)\n",
      "test = pd.read_csv(\"/Users/taposh/workspace/kaggle/bow/testData.tsv\", header=0, delimiter=\"\\t\",quoting=3)    \n",
      "\n",
      "#train = pd.read_csv(\"/Users/taposh/workspace/kaggle/bow/labeledTrainData.tsv\", header=0, \\\n",
      "#                    delimiter=\"\\t\", quoting=3)\n",
      "#test = pd.read_csv(\"/Users/taposh/workspace/kaggle/bow/testData.tsv\", header=0, delimiter=\"\\t\", \\\n",
      "                   #quoting=3 )\n",
      "y = train[\"sentiment\"]  \n",
      "print(\"Cleaning and parsing movie reviews...\\n\")      \n",
      "traindata = []\n",
      "for i in range( 0, len(train[\"review\"])):\n",
      "    traindata.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(train[\"review\"][i], False)))\n",
      "testdata = []\n",
      "for i in range(0,len(test[\"review\"])):\n",
      "    testdata.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(test[\"review\"][i], False)))\n",
      "    #print(testdata)\n",
      "print ('vectorizing... ',) \n",
      "tfv = TfidfVectorizer(min_df=4,  max_features=None, \n",
      "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
      "        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
      "        stop_words = 'english')\n",
      "X_all = traindata + testdata\n",
      "lentrain = len(traindata)\n",
      "\n",
      "print (\"fitting pipeline... \",)\n",
      "tfv.fit(X_all)\n",
      "X_all = tfv.transform(X_all)\n",
      "\n",
      "# RF transform 1st column to numbers\n",
      "#X_all[:,0] = LabelEncoder().fit_transform(X_all[:,0])\n",
      "\n",
      "\n",
      "#for Logit\n",
      "X = X_all[:lentrain]\n",
      "X_test = X_all[lentrain:]\n",
      "\n",
      "\n",
      "#model = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None\n",
      "model = LogisticRegression(penalty='l2', dual=True, tol=0.00001,C=11, fit_intercept=True, intercept_scaling=1,class_weight=None, random_state=None)\n",
      "\n",
      "#model = RandomForestRegressor(n_estimators=150, min_samples_split=1)\n",
      "#model.fit(X, y)\n",
      "#print X\n",
      "#print regressor.predict(X)\n",
      "\n",
      "print(\"25 Fold CV Score: \", np.mean(cross_validation.cross_val_score(model, X, y, cv=25, scoring='roc_auc')))\n",
      "\n",
      "print(\"Retrain on all training data, predicting test labels...\\n\")\n",
      "model.fit(X,y)\n",
      "result = model.predict_proba(X_test)[:,1]\n",
      "#result = model.predict(X_test)\n",
      "print(result)\n",
      "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
      "\n",
      "import csv\n",
      "# Use pandas to write the comma-separated output file\n",
      "output.to_csv('/Users/taposh/workspace/kaggle/bow/Bag_of_Words_model_v11.csv',quoting=3, escapechar=\",\",index=False,encoding='utf-8')\n",
      "#output.to_csv(\"/Users/taposhdr/workspace/decision_science/kaggle/bow/data/Bag_of_Words_model-1.csv\", index=False, quoting=csv.QUOTE_NONE)\n",
      "print(\"Wrote results to Bag_of_Words_model.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cleaning and parsing movie reviews...\n",
        "\n",
        "vectorizing... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "fitting pipeline... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "25 Fold CV Score: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.96408656\n",
        "Retrain on all training data, predicting test labels...\n",
        "\n",
        "[ 0.99110772  0.01987447  0.5607115  ...,  0.38636618  0.96060281\n",
        "  0.6428434 ]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wrote results to Bag_of_Words_model.csv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}