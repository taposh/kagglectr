{
 "metadata": {
  "name": "",
  "signature": "sha256:8c63293855f55f74c23cd7790409db3d3ba3b5d521c6e1ac997095b249ad3a0a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "NLP & Sentiment Analysis"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data Set\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). \"Learning Word Vectors for Sentiment Analysis.\" The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Abstract"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Unsupervised vector-based approaches to semantics can model rich lexical meanings, but\n",
      "they largely fail to capture sentiment information that is central to many word meanings and\n",
      "important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised\n",
      "and supervised techniques to learn word vectors capturing semantic term\u2013document information\n",
      "as well as rich sentiment content. The proposed model can leverage both continuous\n",
      "and multi-dimensional sentiment information as well as non-sentiment annotations.\n",
      "We instantiate the model to utilize the document-level sentiment polarity annotations\n",
      "present in many online documents (e.g. star ratings). We evaluate the model using small,\n",
      "widely used sentiment and subjectivity corpora and find it out-performs several previously\n",
      "introduced methods for sentiment classification. We also introduce a large dataset\n",
      "of movie reviews to serve as a more robust benchmark for work in this area"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "http://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "File descriptions\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "labeledTrainData - The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review.  \n",
      "testData - The test set. The tab-delimited file has a header row followed by 25,000 rows containing an id and text for each review. Your task is to predict the sentiment for each one. \n",
      "unlabeledTrainData - An extra training set with no labels. The tab-delimited file has a header row followed by 50,000 rows containing an id and text for each review. \n",
      "sampleSubmission - A comma-delimited sample submission file in the correct format."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##############################################################################\n",
      "# Taposh Dutta Roy\n",
      "# Sentiment Analysis\n",
      "##############################################################################\n",
      "\n",
      "import os\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.preprocessing import LabelEncoder \n",
      "from sklearn import cross_validation\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import re\n",
      "import nltk\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "from bs4 import BeautifulSoup\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "## Stemming functionality\n",
      "class stemmerUtility(object):\n",
      "    \"\"\"Stemming functionality\"\"\"\n",
      "    @staticmethod\n",
      "    def stemPorter(review_text):\n",
      "        porter = PorterStemmer()\n",
      "        preprocessed_docs = []\n",
      "        for doc in review_text:\n",
      "            final_doc = []\n",
      "            for word in doc:\n",
      "                final_doc.append(porter.stem(word))\n",
      "                #final_doc.append(snowball.stem(word))\n",
      "                #final_doc.append(wordnet.lemmatize(word)) #note that lemmatize() can also takes part of speech as an argument!\n",
      "            preprocessed_docs.append(final_doc)\n",
      "        return preprocessed_docs\n",
      "\n",
      "    \n",
      "## Originally provided by Google BOG Organizers\n",
      "## Modified by Taposh \n",
      "class KaggleWord2VecUtility(object):\n",
      "    \"\"\"KaggleWord2VecUtility is a utility class for processing raw HTML text into segments for further learning\"\"\"\n",
      "\n",
      "    @staticmethod\n",
      "    def review_to_wordlist( review, remove_stopwords=False ):\n",
      "        # 1. Remove HTML\n",
      "        review_text = BeautifulSoup(review).get_text()\n",
      "        # 2. Remove non-letters\n",
      "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
      "        # 2.1 Remove single letters\n",
      "        review_text = re.sub('/(?<!\\S).(?!\\S)\\s*/', '', review_text);\n",
      "        # 3. Convert words to lower case and split them\n",
      "        words = review_text.lower().split()\n",
      "        newwords=[]\n",
      "        for word in words:\n",
      "            if len(word)>2:\n",
      "                newwords.append(word)\n",
      "        # 4. Optionally remove stop words (false by default)\n",
      "        if remove_stopwords:\n",
      "            stops = set(stopwords.words(\"english\"))\n",
      "            newwords = [w for w in newwords if not w in stops]\n",
      "        #\n",
      "        # 5. Return a list of words\n",
      "        return(newwords)\n",
      "\n",
      "    # Define a function to split a review into parsed sentences\n",
      "    @staticmethod\n",
      "    def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
      "        # Function to split a review into parsed sentences. Returns a\n",
      "        # list of sentences, where each sentence is a list of words\n",
      "        #\n",
      "        # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
      "        raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
      "        #\n",
      "        # 2. Loop over each sentence\n",
      "        sentences = []\n",
      "        for raw_sentence in raw_sentences:\n",
      "            # If a sentence is empty, skip it\n",
      "            if len(raw_sentence) > 0:\n",
      "                # Otherwise, call review_to_wordlist to get a list of words\n",
      "                sentences.append( KaggleWord2VecUtility.review_to_wordlist( raw_sentence, \\\n",
      "                  remove_stopwords ))\n",
      "        #\n",
      "        # Return the list of sentences (each sentence is a list of words,\n",
      "        # so this returns a list of lists\n",
      "        return sentences\n",
      "    \n",
      "train = pd.read_csv(\"/Users/taposh/workspace/mlearning/nlp/sentiment/bow/labeledTrainData.tsv\", header=0,delimiter=\"\\t\", quoting=3)\n",
      "test = pd.read_csv(\"/Users/taposh/workspace/mlearning/nlp/sentiment/bow/testData.tsv\", header=0, delimiter=\"\\t\",quoting=3)    \n",
      "\n",
      "#train = pd.read_csv(\"/Users/taposh/workspace/kaggle/bow/labeledTrainData.tsv\", header=0, \\\n",
      "#                    delimiter=\"\\t\", quoting=3)\n",
      "#test = pd.read_csv(\"/Users/taposh/workspace/kaggle/bow/testData.tsv\", header=0, delimiter=\"\\t\", \\\n",
      "                   #quoting=3 )\n",
      "y = train[\"sentiment\"]  \n",
      "print(\"Cleaning and parsing movie reviews...\\n\")      \n",
      "traindata = []\n",
      "for i in range( 0, len(train[\"review\"])):\n",
      "#for i in range(0,10):    \n",
      "    traindata.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(train[\"review\"][i], False)))\n",
      "testdata = []\n",
      "####\n",
      "for i in range(0,len(test[\"review\"])):\n",
      "    testdata.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(test[\"review\"][i], False)))\n",
      "    #print(testdata)\n",
      "print ('vectorizing... ',) \n",
      "tfv = TfidfVectorizer(min_df=2,  max_features=None, \n",
      "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
      "        ngram_range=(1, 2), use_idf=2,smooth_idf=1,sublinear_tf=1,\n",
      "        stop_words = 'english')\n",
      "X_all = traindata + testdata\n",
      "lentrain = len(traindata)\n",
      "\n",
      "\n",
      "print (\"fitting pipeline... \",)\n",
      "tfv.fit(X_all)\n",
      "X_all = tfv.transform(X_all)\n",
      "\n",
      "# RF transform 1st column to numbers\n",
      "#X_all[:,0] = LabelEncoder().fit_transform(X_all[:,0])\n",
      "\n",
      "\n",
      "#for Logit\n",
      "X = X_all[:lentrain]\n",
      "X_test = X_all[lentrain:]\n",
      "\n",
      "\n",
      "#model = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None\n",
      "model = LogisticRegression(penalty='l2', dual=True, tol=0.0001,C=14, fit_intercept=True, intercept_scaling=1,class_weight=None, random_state=None)\n",
      "\n",
      "#http://nbviewer.ipython.org/gist/rjweiss/7577004\n",
      "#model = RandomForestRegressor(n_estimators=150, min_samples_split=1)\n",
      "#model.fit(X, y)\n",
      "#print X\n",
      "#print regressor.predict(X)\n",
      "\n",
      "print(\"25 Fold CV Score: \", np.mean(cross_validation.cross_val_score(model, X, y, cv=36, scoring='roc_auc')))\n",
      "\n",
      "print(\"Retrain on all training data, predicting test labels...\\n\")\n",
      "model.fit(X,y)\n",
      "result = model.predict_proba(X_test)[:,1]\n",
      "#result = model.predict(X_test)\n",
      "print((result))\n",
      "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
      "\n",
      "import csv\n",
      "# Use pandas to write the comma-separated output file\n",
      "output.to_csv('/Users/taposh/workspace/mlearning/nlp/sentiment/bow/Bag_of_Words_model_v17.csv',quoting=3, escapechar=\",\",index=False,encoding='utf-8')\n",
      "#output.to_csv(\"/Users/taposhdr/workspace/decision_science/kaggle/bow/data/Bag_of_Words_model-1.csv\", index=False, quoting=csv.QUOTE_NONE)\n",
      "print(\"Wrote results to csv file\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cleaning and parsing movie reviews...\n",
        "\n",
        "vectorizing... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "fitting pipeline... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "25 Fold CV Score: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.964641955994\n",
        "Retrain on all training data, predicting test labels...\n",
        "\n",
        "[ 0.98821374  0.02077675  0.57377324 ...,  0.38864425  0.96238562\n",
        "  0.69040805]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wrote results to csv file"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###General Stemming Overview\n",
      "  Conflates inflected/derived words to a stem (root)\n",
      "  Intuitive\n",
      "  Stem is not (necessarily) the morphological root\n",
      "  Abate, abated, abatement, abatements, abates might all stem to \u201cabat\u201d\n",
      "  Other stemmers might produce different stems\n",
      "  Crude, imperfect by nature\n",
      "  Ambiguity about correctness\n",
      "#####Applications for stemmers\n",
      "\" Information retrieval (Search engines)\n",
      "! Stem both document indexes and queries\n",
      "! Often can increase recall without decreasing\n",
      "precision\n",
      "! Any situation where one is interested in grouping\n",
      "words into semantically similar sets.\n",
      "\" Other tasks?\n",
      "#####Stemming approaches\n",
      "\" Many different approaches:\n",
      "! Brute force look up\n",
      "! Suffix, affix stripping\n",
      "! Part-of-speech recognition\n",
      "! Statistical algorithms (n-grams, HMM)\n",
      "\" Porter stemmer utilizes suffix stripping\n",
      "\" It does not address prefixes\n",
      "\n",
      "####Porter Stemmer Overview\n",
      "\" Algorithm dates from 1980\n",
      "\" Still the default \u201cgo-to\u201d stemmer\n",
      "\" Excellent trade-off between speed, readability, and\n",
      "accuracy\n",
      "\" Stems using a set of rules, or transformations,\n",
      "applied in a succession of steps\n",
      "\" About 60 rules in 6 steps\n",
      "\" No recursion\n",
      "\n",
      "#####Porter Stemmer Steps\n",
      "\" Step 1: Gets rid of plurals and -ed or -ing suffixes\n",
      "\" Step 2: Turns terminal y to i when there is another\n",
      "vowel in the stem\n",
      "\" Step 3: Maps double suffixes to single ones:\n",
      "-ization, -ational, etc.\n",
      "\" Step 4: Deals with suffixes, -full, -ness etc.\n",
      "\" Step 5: Takes off -ant, -ence, etc.\n",
      "\" Step 6: Removes a final -e "
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Some Test Scores"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "c=16 | 25 Fold CV Score:  0.96409904\n",
      "c=17 | 25 Fold CV Score:  0.96408976\n",
      "c=13 | 25 Fold CV Score:  0.96410064\n",
      "c=09 | 25 Fold CV Score:  0.96406416\n",
      "c=08 | 25 Fold CV Score:  0.96402832\n",
      "c=14 | 25 Fold CV Score:  0.96410448\n",
      "c=14 tf=2 | 25 Fold CV Score:  0.96444656*\n",
      "c=14 tf=2 | 35 Fold CV Score:  0.96461634252\n",
      "c=14 tf=2 | 36 Fold CV Score:  0.964697569423\n",
      "c=14 tf=1 | 25 Fold CV Score:  0.96239056\n",
      "c=12 | 25 Fold CV Score:  0.96409408\n",
      "c=11 | 25 Fold CV Score:  0.96408656\n",
      "c=15 | 25 Fold CV Score:  0.9641014\n",
      "c=14.8 | 25 Fold CV Score:  0.96410384"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}